{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Summer 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Session 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May, 22nd 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ulf Krumnack\n",
    "\n",
    "Institute of Cognitive Science\n",
    "University of Osnabrück"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* exercise sheet 07\n",
    "* principle component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 1 (ML-06 slides 7-12): What is the idea of dimension reduction. When does dimension reduction make sense? Discuss the topic with your neighbor. (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 2 (ML-06 slides 13-19): What is the idea of PCA? How does it work? Name two different ways to describe its objectives. (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 3 (ML-06 slides 20-22): What are eigenfaces? What are they used for? Execute the code in the following cells and try to understand what it is doing. (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4c528e622d59ef6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94ec3cfcfec0e7b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** Import the images from the directory `images/trainimgs` into an numpy array using the function \n",
    "`read_images_from_directory` provided in the cell below. Display the images and the corresponding names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-471f722946eed0b9",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_images_from_directory(directory, suffix, shape):\n",
    "    \"\"\"\n",
    "    Read all images found in DIRECTORY with given file\n",
    "    name SUFFIX. All images should have the same SHAPE,\n",
    "    specified as (rows,columns).\n",
    "    \n",
    "    Returns:\n",
    "        images: A numpy array of shape m*rows*columns (from shape)\n",
    "        names: A list of corresponding image names.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the image array and name list\n",
    "    #images = np.empty((0, *shape))\n",
    "    images = np.empty((0, ) + shape)\n",
    "    names = []\n",
    "\n",
    "    # now loop through all image files in the directory\n",
    "    for file_name in glob.glob(directory + os.sep + '*.' + suffix):\n",
    "        if os.path.isfile(file_name):\n",
    "\n",
    "            # load each image (as double)\n",
    "            img = misc.imread(file_name, mode='F')\n",
    "\n",
    "            # check for correct size\n",
    "            if img.shape == shape:\n",
    "                images = np.append(images, img.reshape((1, ) + shape), axis=0)\n",
    "                names.append(os.path.basename(file_name))\n",
    "            else:\n",
    "                print(\n",
    "                    'warning: Image \"' + file_name +\n",
    "                    '\" with wrong size will be ignored!',\n",
    "                    file=sys.stderr)\n",
    "\n",
    "    return images, names\n",
    "\n",
    "\n",
    "# image file suffix\n",
    "suffix = 'pgm'\n",
    "\n",
    "# image size\n",
    "img_shape = (192, 168)\n",
    "\n",
    "train_imgs, train_names = read_images_from_directory('trainimg','pgm', img_shape)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.gray()\n",
    "for i, n in enumerate(train_names):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(train_imgs[i])\n",
    "    plt.title(n[5:7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3931a24ab8fc5bde",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Principal components are the eigenvalues of the autocorrelation matrix of a dataset. The eigenvalues give the ordering of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0aecb18df86eca832",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca(data):\n",
    "    \"\"\"\n",
    "    Perform principal component analysis.\n",
    "    \n",
    "    Args:\n",
    "        data (ndarray): an k*n dimensional array (k entries with n dimensions)\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Array holding the principal components.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Center the data (subtract the mean along columns).\n",
    "    centered_data = data - data.mean(axis=0)\n",
    "\n",
    "    covariance_matrix = np.cov(centered_data)\n",
    "    \n",
    "    # Computing eigenvalues and eigenvectors of covariance matrix.\n",
    "    # Attention: not always sorted.\n",
    "    eigenvals, eigenvecs = np.linalg.eig(covariance_matrix)\n",
    "    # projection of the data in the new space\n",
    "    pc = eigenvecs.T @ centered_data  \n",
    "\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-26f07d0d9dcfc6f6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** Use PCA to compute the eigenfaces (i.e. the eigenvectors of the face images). Explain what kind of input PCA expects, and how that fits to our images (you may have to `reshape` the images!). Finally, display the eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d074739803952b9a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "face_vecs = train_imgs.reshape((-1, np.prod(img_shape)))\n",
    "eigenfaces = pca(face_vecs)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.gray()\n",
    "for i, eigenface in enumerate(eigenfaces):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(eigenface.reshape(img_shape))\n",
    "    plt.title('Eigenface {}'.format(i))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c54ec5985375835f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** Now project the training face images into the eigenspace to calculate their ”feature vectors”,\n",
    "i.e. a representation with significantly lower dimension. For the projection of the face images,\n",
    "they have to be centered first, i.e. the mean face vector has to be subtracted. Store the mean face in some vector (`mean_face`) and the representation achieved in some array (`face_db`). Finally restore the images from `face_db` and display them alongside the original image. Try out the effect of changing the number of eigenfaces to be used (`num_eigenfaces`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-43eaeder3c18070c8",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# number of eigenfaces to be used\n",
    "num_eigenfaces = 10\n",
    "\n",
    "# Compute the mean face.\n",
    "mean_face = face_vecs.mean(axis=0)\n",
    "\n",
    "# Reduce the number of eigenfaces to achieve more dimensionality reduction.\n",
    "eigenfaces_used = eigenfaces[:num_eigenfaces]\n",
    "\n",
    "# 3. Project images into the eigenface space and store\n",
    "#    them in a \"eigenface database\":\n",
    "face_db = (face_vecs - mean_face) @ eigenfaces_used.T\n",
    "\n",
    "# ... and display the original image and the stored image\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.gray()\n",
    "for i, face in enumerate(face_db):\n",
    "    restored = (face[np.newaxis] @ eigenfaces_used) + mean_face\n",
    "    plt.subplot(5, 8, 2 * i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(train_imgs[i])\n",
    "    plt.title('original')\n",
    "    plt.subplot(5, 8, 2 * i + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(restored.reshape(img_shape))\n",
    "    plt.title('restored')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex-eigenface-recognize",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**d)** Implement the function `recognize_face` that recognizes a face from that database by calculating the euclidean distance of this face feature vector to all of the training feature vectors from the database. The feature vector with the smallest distance represents the winner category. Check your implementation by recognizing the images from the training set (they should be recognized without error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55d01bed03c2d1ca2",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def recognize_face(face, eigenfaces, mean_face, face_db):\n",
    "    \"\"\"\n",
    "    Recognize a face from a face database.\n",
    "    and return the index of the best matching database entry.\n",
    "\n",
    "    The FACE is first centered and projected into the eigeface\n",
    "    space provided by EIGENFACES. Then the best match is found\n",
    "    according to the euclidean distance in the eigenface space.\n",
    "    \"\"\"\n",
    "    index = -1\n",
    "\n",
    "    # center the face\n",
    "    centered = face - mean_face\n",
    "\n",
    "    # and project it into the eigenface space\n",
    "    projected = eigenfaces @ centered\n",
    "\n",
    "    # Now compute the similarity to all known faces\n",
    "    # (comparison is performed in the eigenface space)\n",
    "    distances = cdist(face_db, projected[None, :])\n",
    "    index = distances.argmin()\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "# ... and now check your function on the training set ...\n",
    "def show_recognition_results(imgs, labels, train_imgs, train_labels,\n",
    "                             num_eigenfaces, eigenfaces, mean_face, face_db):\n",
    "    img_shape = imgs[0].shape\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.suptitle(\n",
    "        'Face recognition based on {} principal components'.format(num_eigenfaces))\n",
    "    plt.gray()\n",
    "    for j, img in enumerate(imgs):\n",
    "\n",
    "        # find the best match in the eigenface database\n",
    "        winner = recognize_face(\n",
    "            img.reshape(np.prod(img_shape)), eigenfaces, mean_face, face_db)\n",
    "        name_label = labels[j][5:7]\n",
    "        name_winner = train_labels[winner][5:7]\n",
    "\n",
    "        plt.subplot(5, 8, 2 * j + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.title(labels[j][5:7])\n",
    "\n",
    "        plt.subplot(5, 8, 2 * j + 2)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(train_imgs[winner])\n",
    "        plt.title(('*' if name_label != name_winner else '') + name_winner)\n",
    "    plt.show()\n",
    "    \n",
    "show_recognition_results(train_imgs, train_names,\n",
    "                         train_imgs, train_names,\n",
    "                         num_eigenfaces, eigenfaces_used, \n",
    "                         mean_face, face_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Now classify the images in directory `testimg/`. Try to reduce the number of principal components\n",
    "used. How many PCs are necessary to still achieve perfect classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0167a670acfe187c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "test_imgs, test_names = read_images_from_directory('testimg', suffix, img_shape)\n",
    "\n",
    "show_recognition_results(test_imgs, test_names,\n",
    "                         train_imgs, train_names,\n",
    "                         num_eigenfaces, eigenfaces_used, \n",
    "                         mean_face, face_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sheet 05, Assignment 2: Implement and Apply PCA\n",
    "\n",
    "\n",
    "## Example: the `cars` dataset\n",
    "\n",
    "* simplified from the JSE [2004 New Car and Truck Data](http://www.amstat.org/publications/jse/jse_data_archive.htm))\n",
    "* measurements taken on 97 different cars\n",
    "* eleven features: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches). \n",
    "\n",
    "Goal: visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the cars dataset in cars.csv .\n",
    "cars = np.loadtxt('cars.csv', delimiter=',')\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try to get some initial idea of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "cols = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n",
    "          'Engine size (liters)', 'Number of engine cylinders', \n",
    "          'Engine horsepower', 'City gas mileage' , \n",
    "          'Highway gas mileage', 'Weight (pounds)', \n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "\n",
    "df = pd.DataFrame(cars, columns=cols)\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First step: normalize the data (such that they have a zero mean and a unit standard deviation)\n",
    "\n",
    "$$\\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "* $X$ is an $m \\times n$ matrix with $n=11$ features and $m=97$ samples\n",
    "* $\\mu$ is the mean of that dataset, a $n$-dimensional vector\n",
    "* $\\sigma$ is the standard deviation, again in each of the $n$ feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the data and store them in a variable called cars_norm.\n",
    "cars_norm = (cars - np.mean(cars, axis=0)) / np.std(cars, axis=0)\n",
    "\n",
    "# Alternatively one could use:\n",
    "# import sklearn.preprocessing\n",
    "# cars_norm = sklearn.preprocessing.scale(cars)\n",
    "\n",
    "assert cars_norm.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)\n",
    "assert np.abs(np.sum(cars_norm)) < 1e-10, \"Absolute sum was {} but should be close to 0\".format(np.abs(np.sum(cars_norm)))\n",
    "assert np.abs(np.sum(cars_norm ** 2) / cars_norm.size - 1) < 1e-10, \"The data is not normalized, sum/N was {} not 1\".format(np.sum(cars_norm ** 2) / cars_norm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Find a subspace that maximizes the variance:\n",
    "* determine the eigenvectors of the covariance matrix\n",
    "* for normalized data the covariance matrix is calculated as $C = X^T\\cdot X$\n",
    "* the entry $c_{i,j}$ in $C$ tells how much feature $i$ correlates with feature $j$\n",
    "\n",
    "Note: sometimes the formula $C=X\\cdot X^T$ can be found: depends on how the data is stored!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the autocovariance matrix and store it into autocovar\n",
    "autocovar = cars_norm.T @ cars_norm\n",
    "\n",
    "assert autocovar.shape == (11, 11)\n",
    "\n",
    "# Compute the eigenvalues und eigenvectors and store them into eigenval and eigenvec\n",
    "# (Figure out a function to do this for you)\n",
    "eigenval, eigenvec = np.linalg.eigh(autocovar)\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot the spectrum of the eigenvalues\n",
    "* make sure that they are sorted by their magnitude!\n",
    "* how many principal components should we include based on the spectrum plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Sort eigenvectors (and -values) by descending order of eigenvalues.\n",
    "sort = np.argsort(-eigenval)\n",
    "eigenval = eigenval[sort]\n",
    "eigenvec = eigenvec[:,sort]\n",
    "\n",
    "# To get an idea of the eigenvalues we plot them.\n",
    "figure = plt.figure('Eigenvalue comparison')\n",
    "plt.bar(np.arange(len(eigenval)), eigenval)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Application: Visualization\n",
    "\n",
    "Poject data into a low dimensional subspace (e.g., a 2D space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Project the data down into the two dimensional subspace\n",
    "#proj = cars_norm @ eigenvec[:,0:2]\n",
    "proj = cars_norm @ eigenvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot projected data\n",
    "plt.figure('Data projected onto first two Principal Components')\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-4, 7)\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "# Divide plot into quadrants\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2_d_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### PCA is good\n",
    "* only few points overlap and the points are generally spread out well in the subspace\n",
    "* there is not much trend in the plot which is what we desired, i.e. the axes are not redundant\n",
    "* no clusters can be recognized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_d2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* what kinds of cars are in the four quadrants of the first plot?\n",
    "* was it justifiable that we only considered the first two principle components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ex2_d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "plot the two first principle component vectors as eleven two dimensional points\n",
    "* how are the features are projected into the subspace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot eigenvectors\n",
    "plt.figure('Eigenvector plot',figsize=(12,8))\n",
    "plt.scatter(eigenvec[:,0], eigenvec[:,1])\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "\n",
    "# add labels\n",
    "labels = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n",
    "          'Engine size (liters)', 'Number of engine cylinders', \n",
    "          'Engine horsepower', 'City gas mileage' , \n",
    "          'Highway gas mileage', 'Weight (pounds)', \n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "for label, x, y in zip(labels, eigenvec[:,0], eigenvec[:,1]):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sheet 05, Assignment 3: PCA\n",
    "\n",
    "PCA finds the subspace that captures most of the data variance:\n",
    "* the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components\n",
    "* two steps:\n",
    "  1. a one-dimensional subspace\n",
    "  1. an $m$-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "ex3_a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: a one-dimensional subspace\n",
    "\n",
    "Goal:\n",
    "* determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "* variance of the projected data: $\\vec{p}^{T}C\\vec{p}$ (with $C$ = autocorrelation matrix)\n",
    "* maximize this expression\n",
    "* avoid $\\|\\vec{p}\\|\\to\\infty$: only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. \n",
    "\n",
    "Maximize the expression with this constraint (which can be done using a Lagrangian multiplier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c6ac7b12f3af96f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to maximize the expression\n",
    "$$\\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})$$\n",
    "with respect to $\\vec{p}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-085c07ff9dbddf22",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to find solutions for\n",
    "$$\\frac{\\partial}{\\partial\\vec{p}}\\left[ \\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})\\right] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5e7b6ee88b52fc26",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leads to the equation\n",
    "$$2C\\cdot\\vec{p} = 2\\lambda\\cdot\\vec{p}$$\n",
    "and hence\n",
    "$$C\\cdot\\vec{p} = \\lambda\\cdot\\vec{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0fbe678505c0a94e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "in other words: for a vector $\\vec{p}$ to maximize our expression, it has to be an eigenvector $C$ and $\\lambda$ has to be the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex3_a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By left multiplying with $\\vec{p}^T$ and using the fact that $\\vec{p}^T\\vec{p}=1$, we gain\n",
    "$$\\vec{p}^TC\\vec{p}=\\lambda$$\n",
    "i.e. the projected variance will correspond to the eigenvalue $\\lambda$ and hence is maximized when choosing the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0dae7b280edae9d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: an $m$-dimensional projection space\n",
    "\n",
    "Idea: use an inductive argument:\n",
    "* assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$.\n",
    "* then find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c267bcfbaa5b8122",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, the vector $\\vec{p}_m$ should be linearly independent from $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, as it should define the new $m$-th dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d0882260ceabcba1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This property can be enforced by the (stronger) requirement that $\\vec{p}_{m}$ should be orthogonal to\n",
    "$\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, i.e. \n",
    "\n",
    "$$\\vec{p}_m^T\\vec{p}_{i}=0 \\text{ for } i=1,\\ldots,m-1,$$\n",
    "\n",
    "Espress this constraints by additionalLagrange multipliers $\\eta_1,\\ldots,\\eta_{m-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-85309eade71c8c31",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, the variance in direction $\\vec{p}_m$ is given by\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cc89a3ad757ba8f3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to maximize this value, again with the additional constraint that $\\vec{p}_{m}$ is normalized, i.e.\n",
    "\n",
    "$$\\vec{p}_{m}^T\\vec{p}_m=1,$$\n",
    "\n",
    "which will be expressed by an additional Lagrange multiplier $\\lambda_M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-851e1c6aa05c755c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So in total we want to maximize the function\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_{m})$$\n",
    "with respect to $\\vec{p}_m$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-35f28b5a7a9df7cc",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "i.e. we have to find solutions for\n",
    "\\begin{align}\n",
    "  0\n",
    "  & = \\frac{\\partial}{\\partial\\vec{p}_m}\\left[\\vec{p}_{m}^TC\\vec{p}_{m} \n",
    "  + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i}\n",
    "  + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_m)\\right] \\\\\n",
    "  & = 2C\\vec{p}_m + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_{i} - 2\\lambda_m\\vec{p}_{m}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f4961f383b41352",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Multiplying this equation with $\\vec{p}_{j}^T$ from the left yields (due to the orthogonality constraint)\n",
    "\\begin{align}\n",
    "  0 = \\vec{p}_{j}^T 0\n",
    "  & = \\vec{p}_{j}^T 2C\\vec{p}_m +\n",
    "  \\vec{p}_{j}^T \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_{i} -\n",
    "  \\vec{p}_{j}^T 2\\lambda_m\\vec{p}_{m} \\\\\n",
    "  &= 0 + \\eta_j\\vec{p}_{j}^T \\vec{p}_{j}- 0 \\\\\n",
    "  & = \\eta_j\n",
    "\\end{align}\n",
    "for $j=1,\\ldots,m-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-823ac9b13db07133",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the problem simplifies to\n",
    "$$0 = 2C\\vec{p}_m - 2\\lambda_m\\vec{p}_{m}$$\n",
    "i.e.,\n",
    "$$C\\vec{p}_m = \\lambda_m\\vec{p}_{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2abf663d55906355",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which just means that $\\vec{p}_m$ has to be an eigenvector of the matrix $C$ with eigenvalue $\\lambda_M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1a027412e2a4e91c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, there may be multiple eigenvectors for $C$, so we have to select $\\vec{p}_m$ in a way that it maximizes the variance in direction $\\vec{p}_m$, i.e. the value\n",
    "\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} = \\vec{p}_{m}^T\\lambda_M\\vec{p}_{m} = \\lambda_M.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-331a34ca63371946",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This just means that we have to choose $\\vec{p}_m$ to be the eigenvector with the largest eigenvalue (amongst those not previously selected). This completes the inductive step."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
