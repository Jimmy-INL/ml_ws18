{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Summer 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Session 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May, 17th 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ulf Krumnack\n",
    "\n",
    "Institute of Cognitive Science\n",
    "University of Osnabr√ºck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM Algorithm:\n",
    "* Recap Lecture Slides on the EM algorithm\n",
    "* Sheet 3, Assignment: Expectation Maximization\n",
    "\n",
    "Background reading:\n",
    "* Bishop (2006): *Pattern Recognition and Machine Learning*, chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture Slides on EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 1: Missing value problem. Recap ML04 slides 14-19 and discuss the problem with your neighbor. (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 2: EM algorithm: Study ML04 slides 20-23 and prepare to explain them to the class. (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 3: Convergence of EM: Understand the proof on ML04 slide 24. (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Soft Clustering with Gaussian Mixture\n",
    "\n",
    "\n",
    "Gaussian Mixture models:\n",
    "* can be used for soft clustering since it allows us to express varying degrees of certainty about the membership of individual samples\n",
    "* belong to the most widely used models since Gaussian distributions generally have the property of fitting all different kinds of data reasonably well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mixture Models\n",
    "\n",
    "A mixture model with $K$ components is in general of the form:\n",
    "\n",
    "$$ p(\\mathbf{x}|\\mathbf{\\theta}) = \\sum_{k=1}^K\\pi_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)$$\n",
    "where $\\sum_{k=1}^K\\pi_k = 1$.\n",
    "\n",
    "The probability of observing a dataset $\\mathbf{x}$ given the parameter vector $\\mathbf{\\theta}$ can be expressed as the sum of $K$ individual distributions $p_k$ with parameters $\\mathbf{\\theta}_k \\subseteq {\\theta}$ which are weighted by respective class probabilities $\\pi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "In a Gaussian mixture model one sets\n",
    "\n",
    "$$p_k \\sim \\mathcal{N}(\\mu_k,\\sigma_k)$$\n",
    "\n",
    "with independent parameters $\\mu_k$ and $\\sigma_k$ for each component of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling\n",
    "\n",
    "If we were to randomly pick values for the parameter vector $\\theta$ then we would now have a generative model that can produce naturally clustered data for us, we would just have to sample $\\hat{x} \\sim p(\\mathbf{x}|\\mathbf{\\theta})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Task: Parameter estimation\n",
    "\n",
    "Find the most likely model parameters for the given data. This can be calculated easily by Bayes' Theorem for each model $k$, where the (latent) probability for choosing model $k$ is $p(k|\\mathbf{\\theta}_k) = \\pi_k$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(k|\\mathbf{x},\\mathbf{\\theta})\n",
    "&=\\frac{p(k|\\mathbf{\\theta}_k)p(\\mathbf{x}|k,\\mathbf{\\theta}_k)}{\\sum_{k'=1}^Kp(k'|\\mathbf{\\theta}_{k'})p(\\mathbf{x}|k',\\mathbf{\\theta}_{k'})}\\\\\n",
    "& = \\frac{\\pi_kp_k(\\mathbf{x}|\\mathbf{\\theta}_k)}{\\sum_{k'=1}^K\\pi_{k'}p_{k'}(\\mathbf{x}|\\mathbf{\\theta}_{k'})}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to maximize the log likelihood given as\n",
    "$$\\mathcal{\\ell}(\\mathbf{\\theta})=\\sum_{i=1}^N\\log p(x_i|\\mathbf{\\theta}) = \\sum_{i=1}^N\\log\\left[\\sum_{k=1}^Kp_k(x_i|\\mathbf{\\theta}_k)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All the problems occur because we have a sum inside the logarithm and so we can't pull the logarithm further in towards the densitiy and that is what makes the problem so hard. If we just *ignore* the inner sum we get an expression\n",
    "$$\\mathcal{\\ell}_c(\\mathbf{\\theta}) = \\sum_{i=1}^N\\log p_k(x_i|\\mathbf{\\theta}_k)$$\n",
    "which would be much nicer to compute. But now we have a free floating $k$ in the subscript of our density! Which one of the mixing distributions are we talking about here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kind of all of them at once. But we need one quantity to represent all the distributions. So to get rid of the $k$ we take the expected value with respect to the latent variable $k$ and receive a function that only depends on $\\mathbf{\\theta}$:\n",
    "$$Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) = \\mathbb{E}\\left[\\mathcal{\\ell}_c\\left(\\mathbf{\\theta}\\middle|\\mathcal{\\theta}^{t-1}\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final formula looks as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log\\pi_k + \\sum_i\\sum_k p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "Since $\\theta^{t-1}$ is known at time $t$ we can calculate $p\\left(k\\middle|x_i,\\mathbf{\\theta}^{t-1}\\right)$ with Bayes' Theorem as stated above and replace these expressions with constants $r_{i,k}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sheet 03, Assignment 3: Implement Expectation Maximization for Soft Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Step 1) Load the data\n",
    "\n",
    "\n",
    "Load the provided data set. It is stored in `em_normdistdata.txt`. We call the set $X$ and each individual data $x \\in X$. \n",
    "\n",
    "*Hint:* Figure out a way on how numpy can load text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a1_code",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    Loads the data stored in file_name into a numpy array.\n",
    "    \"\"\"\n",
    "    result = np.loadtxt(file_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = load_data('em_normdistdata.txt')\n",
    "assert data.shape == (200,) , \"The data was not properly loaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Optional:* The data consists of 200 data points drawn from three normal distributions. To get a feeling for the data set you can plot the data with the following cell. Change the number of bins to get a rough idea of how the three distributions might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure('Data overview 1')\n",
    "plt.scatter(data, np.zeros_like(data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a1_test",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure('Data overview 1')\n",
    "plt.hist(data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2) Initialize EM\n",
    "\n",
    "Below is a class definition `NormPDF` which represents the probability density function (pdf) of the normal distribution with an additional parameter $\\alpha$. The class is explained in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NormPDF():\n",
    "    \"\"\"\n",
    "    A representation of the probability density function of the normal distribution\n",
    "    for the EM Algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu=0, sigma=1, alpha=1):\n",
    "        \"\"\"\n",
    "        Initializes the normal distribution with mu, sigma and alpha.\n",
    "        The defaults are 0, 1, and 1 respectively.\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Returns the evaluation of this normal distribution at x.\n",
    "        Does not take alpha into account!\n",
    "        \"\"\"\n",
    "        return np.exp(-(x - self.mu) ** 2 / (2 * self.sigma ** 2)) / (np.sqrt(np.pi * 2) * self.sigma)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A simple string representation of this instance.\n",
    "        \"\"\"\n",
    "        return 'NormPDF({self.mu:.2f},{self.sigma:.2f},{self.alpha:.2f})'.format(self=self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`__init__`: This is the constructor. When a new instance of the class is created this method is used. It takes the parameters `mu`, `sigma`, and `alpha`. Note that if you leave out parameters, they will be set to some default values.\n",
    "So you can create `NormPDF` instances like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "a = NormPDF()             # No parameters: mu = 0, sigma = 1, alpha = 1\n",
    "b = NormPDF(1)            # mu = 1, sigma = 1, alpha = 1\n",
    "c = NormPDF(1, alpha=0.4) # skips sigma but sets alpha, thus: mu = 1, sigma = 1, alpha = 0.4\n",
    "d = NormPDF(0, 0.5)       # mu = 0, sigma = 0.5, alpha = 1\n",
    "e = NormPDF(0, 0.5, 0.9)  # mu = 0, sigma = 0.5, alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`__call__`: This is a very cool feature of Python. By implementing this method one can make an instance *callable*. That basically means one can use it as if it was a function. The `NormPDF` instances can be called with an x value (or a numpy array of x values) to get the evaluation of the normal distribution at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf = NormPDF()\n",
    "print(normpdf(0))\n",
    "print(normpdf(0.5))\n",
    "print(normpdf(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2g",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`__repr__`: This method will be used in Python when one calls `repr(NormPDF())`. As long as `__str__` is not implemented (which you saw in last week's sheet) `str(NormPDF())` will also use this method. This comes in handy for printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2h",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "normpdf2 = NormPDF(1, 0.5, 0.9)\n",
    "print(normpdf1)\n",
    "print([normpdf1, normpdf2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2i",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is also possible to change the values of an instance of the NormPDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2j",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "normpdf1 = NormPDF()\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))\n",
    "\n",
    "normpdf1.mu = 1\n",
    "normpdf1.sigma = 2\n",
    "normpdf1.alpha = 0.9\n",
    "print(normpdf1)\n",
    "print(normpdf1(np.linspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a2k",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that you know how the `NormPDF` class works, it is time for the implementation of the initialization function. Here is the task again:\n",
    "\n",
    "Write a function `gaussians = initialize_EM(data, num_distributions)` to initialize the EM.\n",
    "\n",
    "Each normal distribution $j$ has three parameters: $\\mu_j$ (the mean), $\\sigma_j$ (the standard deviation), $\\alpha_j$ (the proportion of the normal distribution in the mixture, that means $\\sum\\limits_j\\alpha_j=1$).\n",
    "Initialize the three parameters using three random partitions $S_j$ of the data set. Calculate each $\\mu_j$ and $\\sigma_j$ and set $\\alpha_j = \\frac{|S_j|}{|X|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a2_code",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_EM(data, num_distributions):\n",
    "    \"\"\"\n",
    "    Initializes the EM algorithm by calculating num_distributions NormPDFs\n",
    "    from a random partitioning of data. I.e., the data set is randomly\n",
    "    divided into num_distribution parts, and each part is used to initialize\n",
    "    mean, standard deviation and alpha parameter of a NormPDF object.\n",
    "    \n",
    "    Args:\n",
    "        data (array): A collection of data.\n",
    "        num_distributions (int): The number of distributions to return.\n",
    "        \n",
    "    Returns:\n",
    "        A list of num_distribution NormPDF objects, initialized from a\n",
    "        random partioning of the data.\n",
    "    \"\"\"\n",
    "    # generate len(data) many random integers between 0 and num_distributions\n",
    "    partition_mapping = np.random.randint(0, num_distributions, len(data))\n",
    "    # initialize num_distributions many NormPDFs\n",
    "    gaussians = [NormPDF() for i in range(num_distributions)]\n",
    "    \n",
    "    # calculate the mean, standard deviation and alpha depending on the partition mapping\n",
    "    for index, gaussian in enumerate(gaussians):\n",
    "        gaussians[index].mu = np.mean(data[partition_mapping == index])\n",
    "        gaussians[index].sigma = np.std(data[partition_mapping == index])\n",
    "        gaussians[index].alpha = len(data[partition_mapping == index]) / len(data)\n",
    "\n",
    "    return gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a2_code",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "normpdfs_ = initialize_EM(np.linspace(-1, 1, 100), 2)\n",
    "assert len(normpdfs_) == 2, \"The number of initialized distributions is not correct.\"\n",
    "# 1e-10 is 0.0000000001\n",
    "assert abs(1 - sum([normpdf.alpha for normpdf in normpdfs_])) < 1e-10 , \"Sum of all alphas is not 1.0!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls = np.linspace(data.min(),data.max(),100)\n",
    "\n",
    "plt.figure()\n",
    "for pdf in normpdfs_:\n",
    "    plt.plot(ls,pdf(ls),label=pdf)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3) Implement the expectation step\n",
    "\n",
    "Perform a soft classification of the data samples with the normal distributions. That means: Calculate the likelihood that a data sample $x_i$ belongs to distribution $j$ given parameters $\\mu_j$ and $\\sigma_j$. Or in other words, what is the likelihood of $x_i$ to be drawn from $N_j(\\mu_j, \\sigma_j)$? When you got the likelihood, weight the result by $\\alpha_j$.\n",
    "\n",
    "As a last step normalize the results such that the likelihoods of a data sample $x_i$ sum up to $1$.\n",
    "\n",
    "*Hint:* Store the data in a different array before you normalize it to not run into problems with partly normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a3_code",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def expectation_step(gaussians, data):\n",
    "    \"\"\"\n",
    "    Performs the expectation step of the EM.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        \n",
    "    Returns:\n",
    "        An array of shape (len(data), len(gaussians))\n",
    "        which contains normalized likelihoods for each sample\n",
    "        to denote to which of the normal distributions it \n",
    "        most likely belongs to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculates the likelihoods of the samples per \n",
    "    # distribution and weights the results by alpha.\n",
    "    tmp = np.empty((len(data), len(gaussians)))\n",
    "    for j, N in enumerate(gaussians):\n",
    "        tmp[:,j] = N.alpha * N(data)\n",
    "\n",
    "    # Normalize the results.\n",
    "    expectation = np.zeros_like(tmp)\n",
    "    for j, _ in enumerate(gaussians):\n",
    "        expectation[:,j] = tmp[:,j] / np.sum(tmp, 1)\n",
    "\n",
    "    return expectation\n",
    "    \n",
    "\n",
    "assert expectation_step([NormPDF(), NormPDF()], np.linspace(-2, 2, 100)).shape == (100, 2) , \"Shape is not correct!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4) Implement the maximization step\n",
    "\n",
    "In the maximization step each $\\mu_j$, $\\sigma_j$ and $\\alpha_j$ is updated. First calculate the new means:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij}x_i$$\n",
    "\n",
    "That means $\\mu_j$ is the weighted mean of all samples, where the weight is their likelihood of belonging to distribution $j$.\n",
    "\n",
    "Then calculate the new $\\sigma_j$. Each new $\\sigma_j$ is the standard deviation of the normal distribution with the new $\\mu_j$, so for the calculation you already use the new $\\mu_j$:\n",
    "\n",
    "$$\\sigma_j = \\sqrt{ \\frac{1}{\\sum\\limits_{i=1}^{|X|} p_{ij}} \\sum\\limits_{i=1}^{|X|} p_{ij} \\left(x_i - \\mu_j\\right)^2 }$$\n",
    "\n",
    "To calculate the new $\\alpha_j$ for each distribution, just take the mean of $p_j$ for each normal distribution $j$.\n",
    "\n",
    "**Caution:** For the next step it is necessary to know how much all $\\mu$ and $\\sigma$ changed. For that the function `maximization_step` should return a numpy array of those (absolute) changes. For example if $\\mu_0$ changed from 0.1 to 0.15, $\\sigma_0$ from 1 to 0.9, $\\mu_1$ from 0.5 to 0.6, and $\\sigma_1$, $\\mu_2$, and $\\sigma_2$ stayed the same, we expect the function to return `np.array([0.05, 0.1, 0.1, 0, 0, 0])` (however, the order is not important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a4_code",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def maximization_step(gaussians, data, expectation):\n",
    "    \"\"\"\n",
    "    Performs the maximization step of the EM.\n",
    "    Modifies the gaussians by updating their mus and sigmas.\n",
    "    \n",
    "    Args:\n",
    "        gaussians (list): A list of NormPDF objects.\n",
    "        data (array): The data vector.\n",
    "        expectation (array): The expectation values for data element\n",
    "            (as computed by expectation_step()).\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of absolute changes in any mu or sigma, \n",
    "        that means the returned array has twice as many elements as\n",
    "        the supplied list of gaussians.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "\n",
    "    for j, N in enumerate(gaussians):\n",
    "        # calculate new parameters\n",
    "        # @ is the matrix multiplication (behaves like numpy.matlib.matmul)\n",
    "        mu = expectation[:,j] @ data / np.sum(expectation[:,j])\n",
    "        sigma = np.sqrt((expectation[:,j] @ (data - mu) ** 2) / np.sum(expectation[:,j]))\n",
    "        alpha = np.mean(expectation[:,j])\n",
    "        \n",
    "        # append relevant changes\n",
    "        changes += [np.abs(N.mu - mu)]\n",
    "        changes += [np.abs(N.sigma - sigma)]\n",
    "        \n",
    "        # update gaussian\n",
    "        N.mu = mu\n",
    "        N.sigma = sigma\n",
    "        N.alpha = alpha\n",
    "\n",
    "    return np.array(changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "3a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 5) Perform the complete EM and plot your results:**\n",
    "\n",
    "Initialize three normal distributions whose parameters will be changed iteratively by the EM to converge close to the original distributions.\n",
    "\n",
    "Build a loop around the iterative procedure of expectation and maximization which stops when the changes in all $\\mu_j$ and $\\sigma_j$ are sufficiently small enough.\n",
    "\n",
    "Plot your results after each step and mark which data points belong to which normal distribution. If you don't get it to work, just plot your final solution.\n",
    "\n",
    "*Hint:* Remember to load the data and initialize the EM before the loop.\n",
    "\n",
    "*Hint:* A function `plot_intermediate_result` to plot your result after each step is already defined in the next cell. Take a look at what arguments it takes and try to use it in your loop.\n",
    "\n",
    "*Hint:* To plot your final result the first three images and corresponding code examples on the tutorial of [`plt.plot(...)`](http://matplotlib.org/users/pyplot_tutorial.html) should help you.\n",
    "\n",
    "*Optional:* Run the code multiple times. If your results are changing, use `np.random.seed(2)` in the beginning of the cell to get consistent results (any other integer will work as well, but 2 has some good results for the example solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "3a5_code",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Sets the random seed to a fix value to make results consistent\n",
    "np.random.seed(2)\n",
    "\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y', 'k'])\n",
    "figure, axis = plt.subplots(1)\n",
    "axis.set_xlim(-5, 5)\n",
    "axis.set_ylim(-0.2, 4)\n",
    "axis.set_title('Intermediate Results')\n",
    "plt.figure('Final Result')\n",
    "\n",
    "def plot_intermediate_result(gaussians, data, mapping):\n",
    "    \"\"\"\n",
    "    Gets a list of gaussians and data input. The mapping\n",
    "    parameter is a list of indices of gaussians. Each value\n",
    "    corresponds to the data value at the same position and \n",
    "    maps this data value to the proper gaussian.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    if len(axis.lines):\n",
    "        for j, N in enumerate(gaussians):\n",
    "            axis.lines[j * 2].set_xdata(x)\n",
    "            axis.lines[j * 2].set_ydata(N(x))\n",
    "            axis.lines[j * 2 + 1].set_xdata(data[mapping == j])\n",
    "            axis.lines[j * 2 + 1].set_ydata([0] * len(data[mapping == j]))\n",
    "    else:\n",
    "        for j, N in enumerate(gaussians):\n",
    "            axis.plot(x, N(x), data[mapping == j], [0] * len(data[mapping == j]), 'x', color=next(colors), markersize=5)\n",
    "    figure.canvas.draw()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "# Perform the initialization.\n",
    "data = load_data('em_normdistdata.txt')\n",
    "gaussians = initialize_EM(data, 3)\n",
    "\n",
    "# Loop until the changes are small enough.\n",
    "eps = 0.005\n",
    "changes = [float('inf')] * 2\n",
    "while max(changes) > eps:\n",
    "    # Iteratively apply the expectation step, followed by the maximization step.\n",
    "    expectation = expectation_step(gaussians, data)\n",
    "    changes = maximization_step(gaussians, data, expectation)\n",
    "\n",
    "    # Optional: Calculate the parameters to update the plot and call the function to do it.\n",
    "    plot_intermediate_result(gaussians, data, np.argmax(expectation, 1))\n",
    "\n",
    "\n",
    "# Plot your final result and print the final parameters.\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "plt.plot(x, gaussians[0](x), 'r', x, gaussians[1](x), 'g', x, gaussians[2](x), 'b')\n",
    "print(gaussians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding optimal parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{arg\\,max}$\n",
    "In the lecture you saw a proof that if we choose\n",
    "$$\\mathbf{\\theta}^t = \\argmax_{\\mathbf{\\theta}} Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$$\n",
    "that the likelihood of the parameter is non-decreasing then. So we want to maximize $Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$ for the parameters $\\left(\\pi_1\\dots,\\pi_K\\right)$ and $\\theta = \\left(\\mu_1,\\dots,\\mu_K,\\sigma_1,\\dots,\\sigma_K\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So your job is to take the derivative of \n",
    "$$\\begin{align}\n",
    "Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) &= \\sum_i\\sum_k r_{i,k}\\log\\pi_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\n",
    "\\end{align}$$\n",
    "with respect to these variables, to set it equal to 0 and to solve for the value that you are currently maximizing for. You only have to do this for the one dimensional case, i.e. \n",
    "$$p_k(x_i|\\mathbf{\\theta}_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate the maximizer for the $\\mu_k$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to compute the derivative\n",
    "$$\\frac{\\partial}{\\partial \\mu_k}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which amounts to\n",
    "$$\\frac{\\partial}{\\partial \\mu_k}\\left( \\sum_i\\sum_k r_{i,k}\\log\\pi_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and sind $\\log\\pi_k$ does not depend on $\\mu_k$ simplifies to\n",
    "$$\\frac{\\partial}{\\partial \\mu_k}\\sum_i\\sum_k r_{i,k}\\log p_k(x_i|\\mathbf{\\theta})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The derivative of every summand is\n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial}{\\partial \\mu_k}\\log p_k(x_i|\\mathbf{\\theta}) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\mu_k}\\log \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\mu_k}\\left(\\log \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} + \\log\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)\\right) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\mu_k}\\left(\\log \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} - {\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)  \\\\\n",
    "& = 0 + \\frac{1}{2\\sigma_k^2}2\\left(x_i-\\mu_k\\right) \\\\\n",
    "& = \\frac{1}{\\sigma_k^2}\\left(x_i-\\mu_k\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So the derivative of $Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)$ is\n",
    "\\begin{align} \n",
    "\\frac{\\partial}{\\partial \\mu_k}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right)\n",
    "&= \\frac{\\partial}{\\partial \\mu_k}\\sum_i\\sum_k r_{i,k}\\log p_k(\\mathbf{x}_i|\\mathbf{\\theta}) \\\\\n",
    "&= \\sum_i r_{i,k}\\frac{1}{\\sigma_k^2}\\left(x_i-\\mu_k\\right) \\\\\n",
    "&= \\frac{1}{\\sigma_k^2}\\sum_i r_{i,k}(x_i-\\mu_k) \\stackrel{!}{=} 0 \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\sum_i r_{i,k}x_i = \\mu_k\\sum_i r_{i,k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and so we get for $\\mu_k$:\n",
    "$$\\Leftrightarrow \\mu_k = \\frac{\\sum_i r_{i,k}x_i}{\\sum_ir_{i,k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate the maximizer for the $\\sigma_k^2$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{align}\n",
    "& \\frac{\\partial}{\\partial \\sigma_k^2}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) \\\\\n",
    "& =\\frac{\\partial}{\\partial \\sigma_k^2}\\left( \\sum_i\\sum_k r_{i,k}\\log\\pi_k + \\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and as the first term does not depend on $\\sigma_k^2$:\n",
    "\n",
    "$$=\\frac{\\partial}{\\partial \\sigma_k^2}\\left(\\sum_i\\sum_k r_{i,k}\\log p_k\\left(x_i\\middle|\\mathbf{\\theta}\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to compute the derivative for\n",
    "$$\\begin{align}\n",
    "\\log p_k(x_i|\\mathbf{\\theta}_k) \n",
    "& = \\log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)\\right) \\\\\n",
    "& = \\log \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} +\n",
    "\\log \\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This amounts to\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\sigma_k^2}\\log \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\n",
    "&= -\\frac{1}{2}\\log\\left(\\sigma_k^2\\right) \\\\\n",
    "\\frac{\\partial}{\\partial \\sigma_k^2}\\log \\exp\\left({-\\frac{\\left(x_i-\\mu_k\\right)^2}{2\\sigma_k^2}}\\right)\n",
    "&= -\\frac{1}{2\\sigma_k^4}\\left(x_i-\\mu_k\\right)^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So in the end we get \n",
    "\\begin{align}\n",
    "& \\frac{\\partial}{\\partial \\sigma_k^2}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) \\\\\n",
    "& = \\sum_i r_{i,k}\\left(\\frac{1}{\\sigma_k^4}(x_i-\\mu_k)^2 -\\frac{1}{\\sigma_k^2}\\right) \\stackrel{!}{=} 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And hence we can conclude:\n",
    "$$\\Leftrightarrow \\sum_i r_{i,k}(x_i-\\mu_k)^2 = \\sigma_k^2\\sum_ir_{i,k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\sigma_k^2 = \\frac{\\sum_i r_{i,k}(x_i-\\mu_k)^2}{\\sum_ir_{i,k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate the maximizer for the $\\pi_k$ (You need the ensure $\\sum_k\\pi_k =1$. You can either use a Lagrangian Multiplier for this or use the formula to express one of the $\\pi_i$ in terms of all the others):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{align} \n",
    "0 & =\n",
    "\\frac{\\partial}{\\partial \\pi_k}\\left(Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) - \\lambda \\left(\\sum_k \\pi_k - 1\\right)\\right)\\\\\n",
    "&= \\sum_i \\frac{r_{i,k}}{\\pi_k} - \\lambda\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\sum_i \\frac{r_{i,k}}{\\lambda} = \\pi_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\lambda}Q\\left(\\mathbf{\\theta},\\mathbf{\\theta}^{t-1}\\right) + \\lambda \\left(\\sum_k \\pi_k - 1\\right) &= \\sum_k \\pi_k - 1 \\stackrel{!}{=} 0 \\Leftrightarrow \\sum_k \\pi_k = 1 \\\\\n",
    "\\Rightarrow \\frac{1}{\\lambda}\\sum_k\\sum_i r_{i,k} &= 1 \\\\\n",
    "\\Rightarrow \\pi_k &= \\frac{1}{N}\\sum_i r_{i,k}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because of popular request: If we don't want to use the Lagrangian the notation becomes a bit more cumbersome but the overall strategy remains the same.\n",
    "\n",
    "$$\\sum_{k=1}^K\\pi_k = 1 \\Leftrightarrow \\pi_K = 1 - \\sum_{k=1}^{K-1}\\pi_k$$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\pi_k}Q &= \\frac{\\partial}{\\partial \\pi_k}\\sum_{i=1}^N \\left(\\sum_{k=1}^{K-1}r_{ik}\\log\\pi_k + r_{ik}\\log(1-\\sum_{k=1}^{K-1}\\pi_k)\\right) \\\\\n",
    "&= \\sum_{i=1}^N\\left(\\frac{r_{ik}}{\\pi_k} - \\frac{r_{ik}}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right) \\stackrel{!}{=} 0 \\\\\n",
    "\\Leftrightarrow \\pi_k &= \\left(\\sum_{i=1}^{N}r_{ik}\\right)\\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we now sum over $k$ we get\n",
    "\\begin{align}\n",
    "\\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}} \\sum_{k=1}^K\\sum_{i=1}^Nr_{ik} &= 1 \\\\\n",
    "\\Leftrightarrow \\frac{\\left(1 - \\sum_{k=1}^{K-1}\\pi_k\\right)}{\\sum_{i-1}^Nr_{iK}} &= \\frac{1}{N}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
