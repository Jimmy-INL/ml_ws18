{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Summer 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Session 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May, 29nd 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ulf Krumnack\n",
    "\n",
    "Institute of Cognitive Science\n",
    "University of Osnabr√ºck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* exercise sheet 08\n",
    "* principle component analysis (PCA)\n",
    "* multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sheet 05, Assignment 3: PCA\n",
    "\n",
    "PCA finds the subspace that captures most of the data variance:\n",
    "* the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components\n",
    "* two steps:\n",
    "  1. a one-dimensional subspace\n",
    "  1. an $m$-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "ex3_a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: a one-dimensional subspace\n",
    "\n",
    "Goal:\n",
    "* determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "* variance of the projected data: $\\vec{p}^{T}C\\vec{p}$ (with $C$ = autocorrelation matrix)\n",
    "* maximize this expression\n",
    "* avoid $\\|\\vec{p}\\|\\to\\infty$: only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. \n",
    "\n",
    "Maximize the expression with this constraint (which can be done using a Lagrangian multiplier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c6ac7b12f3af96f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to maximize the expression\n",
    "$$\\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})$$\n",
    "with respect to $\\vec{p}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-085c07ff9dbddf22",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to find solutions for\n",
    "$$\\frac{\\partial}{\\partial\\vec{p}}\\left[ \\vec{p}^T C\\vec{p} + \\lambda(1-\\vec{p}^T\\vec{p})\\right] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5e7b6ee88b52fc26",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leads to the equation\n",
    "$$2C\\cdot\\vec{p} = 2\\lambda\\cdot\\vec{p}$$\n",
    "and hence\n",
    "$$C\\cdot\\vec{p} = \\lambda\\cdot\\vec{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0fbe678505c0a94e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "in other words: for a vector $\\vec{p}$ to maximize our expression, it has to be an eigenvector of $C$ and $\\lambda$ has to be the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex3_a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By left multiplying with $\\vec{p}^T$ and using the fact that $\\vec{p}^T\\vec{p}=1$, we gain\n",
    "$$\\vec{p}^TC\\vec{p}=\\lambda$$\n",
    "i.e. the projected variance will correspond to the eigenvalue $\\lambda$ and hence is maximized when choosing the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0dae7b280edae9d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: an $m$-dimensional projection space\n",
    "\n",
    "Idea: use an inductive argument:\n",
    "* assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$.\n",
    "* then find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c267bcfbaa5b8122",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, the vector $\\vec{p}_m$ should be linearly independent from $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, as it should define the new $m$-th dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d0882260ceabcba1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This property can be enforced by the (stronger) requirement that $\\vec{p}_{m}$ should be orthogonal to\n",
    "$\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, i.e. \n",
    "\n",
    "$$\\vec{p}_m^T\\vec{p}_{i}=0 \\text{ for } i=1,\\ldots,m-1,$$\n",
    "\n",
    "Espress this constraints by additionalLagrange multipliers $\\eta_1,\\ldots,\\eta_{m-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-85309eade71c8c31",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, the variance in direction $\\vec{p}_m$ is given by\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cc89a3ad757ba8f3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to maximize this value, again with the additional constraint that $\\vec{p}_{m}$ is normalized, i.e.\n",
    "\n",
    "$$\\vec{p}_{m}^T\\vec{p}_m=1,$$\n",
    "\n",
    "which will be expressed by an additional Lagrange multiplier $\\lambda_M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-851e1c6aa05c755c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So in total we want to maximize the function\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i} + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_{m})$$\n",
    "with respect to $\\vec{p}_m$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-35f28b5a7a9df7cc",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "i.e. we have to find solutions for\n",
    "\\begin{align}\n",
    "  0\n",
    "  & = \\frac{\\partial}{\\partial\\vec{p}_m}\\left[\\vec{p}_{m}^TC\\vec{p}_{m} \n",
    "  + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_m^T\\vec{p}_{i}\n",
    "  + \\lambda_m(1-\\vec{p}_{m}^T\\vec{p}_m)\\right] \\\\\n",
    "  & = 2C\\vec{p}_m + \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_{i} - 2\\lambda_m\\vec{p}_{m}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9f4961f383b41352",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Multiplying this equation with $\\vec{p}_{j}^T$ from the left yields (due to the orthogonality constraint)\n",
    "\\begin{align}\n",
    "  0 = \\vec{p}_{j}^T 0\n",
    "  & = \\vec{p}_{j}^T 2C\\vec{p}_m +\n",
    "  \\vec{p}_{j}^T \\sum_{i=1}^{m-1}\\eta_i\\vec{p}_{i} -\n",
    "  \\vec{p}_{j}^T 2\\lambda_m\\vec{p}_{m} \\\\\n",
    "  &= 0 + \\eta_j\\vec{p}_{j}^T \\vec{p}_{j}- 0 \\\\\n",
    "  & = \\eta_j\n",
    "\\end{align}\n",
    "for $j=1,\\ldots,m-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-823ac9b13db07133",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the problem simplifies to\n",
    "$$0 = 2C\\vec{p}_m - 2\\lambda_m\\vec{p}_{m}$$\n",
    "i.e.,\n",
    "$$C\\vec{p}_m = \\lambda_m\\vec{p}_{m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2abf663d55906355",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which just means that $\\vec{p}_m$ has to be an eigenvector of the matrix $C$ with eigenvalue $\\lambda_M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1a027412e2a4e91c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, there may be multiple eigenvectors for $C$, so we have to select $\\vec{p}_m$ in a way that it maximizes the variance in direction $\\vec{p}_m$, i.e. the value\n",
    "\n",
    "$$\\vec{p}_{m}^TC\\vec{p}_{m} = \\vec{p}_{m}^T\\lambda_M\\vec{p}_{m} = \\lambda_M.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-331a34ca63371946",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This just means that we have to choose $\\vec{p}_m$ to be the eigenvector with the largest eigenvalue (amongst those not previously selected). This completes the inductive step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 1 (ML-07 slides 30-40): What is a perceptron? What is the perceptron learning rule? What problems can be solved by a perceptron? (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 2: What is an MLP? How does it operate? What problems can be solved with an MLP?. (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise 3 (ML-07 slides 20-22): What does the backpropagation algorithm do? What is its goal? How does it work? (10 min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
