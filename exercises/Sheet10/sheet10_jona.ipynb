{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e2554283bc4968a865d3d6047cc3fbd",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55407af6eb9f6570e6a540573ac61119",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 17, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfd0795e0b8576d6c7c8ade79e973fd",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Classification [8 Points]\n",
    "\n",
    "In the lecture (ML-09 Slides 7ff) several types of classifiers have been introduced. In this assignment you will explore differences and similarities between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "352452f5e95d33117c0c47fc92057925",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### a) LDA\n",
    "\n",
    "How does the LDA classifier work? What restrictions have to be fullfilled by the data for this method to work and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1d21daa6f51ede147cbeeee2bb27ff53",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "Linear discrminant analysis is a procedure to find lines for which if the data is projected on them the categories (of the data) are separeted best. There number of lines is determined by the number of categories which have to be separeted. The procedure of LDA works as follows: The center of mass of the different categories is computed. The differnces of the center of mass is computed. This vector is divided by the sum of all variances. This gives us a vector whith the desired properties. The vector orthgonal of the center of this vector (in the direction of the projection) is the separation line. \n",
    "\n",
    "The data has to be linaer separable. Otherwise LDA will not result in a good separation of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dcbdd128192017afa564e292c50608bb",
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### b) Nearest Neighbor\n",
    "\n",
    "How does the nearest neighbor classifier work? When would you use it and how is it trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a72eff8ab8930dd0c611df3689e7f2b3",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "The nearest neighbor classifier is a procedeure which chooses the class for a data point by suming the classes of k nearest neighors divided by the number of those neighbors. For discrete outputs the closest discrete value to the computed one is chosen. K is a parameter of the algorithm. The algorithm can be modified by also accounting for the distance of the k neighbors as a weight (data point minus neighbor). This algorithm is called distance-weighted k-nearest neoghbor algorithm. \n",
    "\n",
    "The k-nearest neighbor algorithm is trained very fast (actually no training at all), but requires a lot of memory since all examples have to be memorized all the time for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40a9572be114c03d67c097c9715ff6c1",
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### c) Support Vector Machines\n",
    "\n",
    "Name some differences between a SVM and a MLP. When would you use which?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8c8116ffb08dc1a3ed3655a57894a04b",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "In tarining time performance the SVM is slower to train than the MLP. This is the case since the SVM has to solve an optimization problem with a high number of variables. Whereas the backpropagation of the MLP is faster to compute. Also MLP is easier to parallelize. The SVM is also best suited for binary classification problems and adaptation to multi-class problems is payed by performance. \n",
    "\n",
    "The classification performance of the SVM is better than the MLP. This is because the MLP is trained on the squared error of the training data to the output. While the SVM is trained for an explicit determination of the decision boundaries directly from the training data. \n",
    "\n",
    "According to this I would chose the MLP if training performance matters more and SVM if classification performance is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46ea19782cdcc0d53854ba122409aeb2",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### d) Random forests\n",
    "\n",
    "Explain in your own words the concept of a *random forest*. What is meant by *bagging of trees* and *bagging of features* and what are the respective benefits? How do radom forests allow for *parallelization* in *training* and *classification*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8ae98ceb3aa2230a636bbd5a9382e215",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "Random forest is built on the algorithm of decision trees. Decision trees are simple classifiers but tend to overfitting. The idea of random forest is to seperate the data in to random chunks and train decison trees on each chunk. The classified output of a random forest is the weighted sum of the output of all decison trees. This method prevents from overfitting of each of those decison trees. \n",
    "\n",
    "Bagging of trees refers to the idea of multiple trees (a bag full of trees) according to the random subsets of data. Bagging of features refers to the idea that for each decision tree the beast node is not chosen from all features but also from a random subset of features. \n",
    "\n",
    "Since the decision trees can be trained and can classify independent of each other they allow for parallelization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa1cd91fb43d57ef899c8956281a439f",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Reinforcement Learning [12 Points]\n",
    "\n",
    "In this assignment you will have a look at the Q-Learning algorithm described in the lecture (ML-10 Slide 18). For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below the code there are some questions! You also find a free-code field for a complete own implementation. You may use your own test mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x (int):            x dimension of the field\n",
    "        y (int):            y dimension of the field \n",
    "        num_rewards (int):  the number of rewards that should be randomly placed\n",
    "        max_reward (int):   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        ndarray: A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    field = np.zeros((y,x), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f7f8ca74bab8c38791891ac0862645ca",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 7.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, actions, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            field (ndarray):  the map\n",
    "            actions (list):   the available actions\n",
    "            gamma (float):    the gamma in the lecture slides\n",
    "        \n",
    "        Returns:\n",
    "            QLearning: An instance that can be used for Q-Learning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field.\n",
    "        self.field = field\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Remember the map extend for further navigation.\n",
    "        self.map_y = self.field.shape[0]\n",
    "        self.map_x = self.field.shape[1]\n",
    "        \n",
    "        # Create q value matrix.\n",
    "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
    "\n",
    "        # Start on a random position in the field.\n",
    "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
    "\n",
    "\n",
    "    def get_coordinates(self, position, action):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain action, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            position (pair):  the current position\n",
    "            action (string):  the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            pair of int: the updated coordinates\n",
    "        \"\"\"\n",
    "        # return the right new coordinates depending on the position\n",
    "        # YOUR CODE HERE\n",
    "        y, x = position\n",
    "        \n",
    "        if action == 'up' and x < self.map_x-1:\n",
    "            x += 1\n",
    "        elif action == 'down' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'right' and y < self.map_y-1:\n",
    "            y += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        return (y,x)\n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # Select a random action that should be performed next.\n",
    "        # Be careful to handle the case where you hit the wall!\n",
    "        # YOUR CODE HERE\n",
    "        action = rand.choice(self.actions)\n",
    "        \n",
    "\n",
    "        # Receive the reward for the new position from the field.\n",
    "        # YOUR CODE HERE\n",
    "        reward = self.field[self.pos[0],self.pos[1]]\n",
    "        \n",
    "        \n",
    "        # Update the q-value for the performed action.\n",
    "        # YOUR CODE HERE\n",
    "        new_pos = self.get_coordinates(self.pos,action)\n",
    "        q_max = np.max(self.q[:,new_pos[0],new_pos[1]]) \n",
    "        \n",
    "        #if we hit a \"wall\" we do not need to update\n",
    "        if new_pos == self.pos: return\n",
    "        \n",
    "        #otherwise we update\n",
    "        self.q[self.actions.index(action),self.pos[0],self.pos[1]] = reward + self.gamma*q_max\n",
    "\n",
    "        # Update the position of the player to the new field.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        self.pos = new_pos\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current state.\n",
    "        \"\"\"\n",
    "        fig_player = plt.figure('QLearning State')\n",
    "\n",
    "        for i, direc in enumerate(ACTIONS):\n",
    "            plt.subplot(3,3,2*i+2)\n",
    "            plt.axis('off')\n",
    "            plt.title(direc)\n",
    "            plt.imshow(self.q[i,:,:], interpolation = 'None')\n",
    "\n",
    "        fig_player.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "03b45ab8a6e3f9240530299d26fd5fea",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 5\n",
    "m_y = 4\n",
    "\n",
    "steps = 200\n",
    "\n",
    "ACTIONS = ['up','left','right','down']  # Those are the availabe actions for the QLearning.\n",
    "FIELD = generate_field(m_x, m_y, 5, 10) # The field that is used for learning.\n",
    "\n",
    "# Plotting the generated field\n",
    "figure = plt.figure('Field')\n",
    "plt.axis('off')\n",
    "plt.imshow(FIELD, interpolation='none')\n",
    "figure.canvas.draw()\n",
    "\n",
    "# Generate a QLearning instance with the right parameters.\n",
    "# YOUR CODE HERE\n",
    "GAMMA = 0.2\n",
    "player = QLearning(FIELD, ACTIONS,GAMMA)\n",
    "\n",
    "\n",
    "# Now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance.\n",
    "for i in range(steps):     \n",
    "    player.update()\n",
    "    player.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "abb7d97c008b533c8d2649cd3e54f3c6",
     "grade": false,
     "grade_id": "ex2_x",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Explain in your own words, how the algorithm works. What is depicted on the resulting plots. How can an action policy be derived from these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b61f070ffb0153e5dd4767692d90b83",
     "grade": true,
     "grade_id": "ex2c_solution",
     "locked": false,
     "points": 3.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "The Q Learning algorithm works as follows:\n",
    "The aim is to learn the best action for a current state. But unlike supervised learning their are no actions avaialable for the learning procedure. The best actions are determined by a reward and the best action of the state to which the action will lead us. This is like the problem of the henn and the egg since we do not know the best action of the state the action will lead us. But in Q Learning this solved by randomly chosing an action and updating its value with the best value for a future action found in the future state to which the action will lead us. By this procedure the algorithm figures out the best action for each state in an iterative learning procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ebf1427a0356c02f45ce383edafe01b5",
     "grade": false,
     "grade_id": "ex2_0",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm (instead of completing the code above). Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3580d7ecdc9edca9b27f5493dde82a0b",
     "grade": true,
     "grade_id": "ex2_alternative",
     "locked": false,
     "points": 0.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
