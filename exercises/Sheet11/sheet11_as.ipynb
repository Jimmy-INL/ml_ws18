{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9e7d36d014ba44abbf2fe8283bbe76c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f06f9d6c2384d2f2a11309031ea33b68",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 24, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c989ef972a476392e000cb61e20d673f",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Uncertainty and probability [6 Points]\n",
    "\n",
    "This exercise will focus on concepts introduced in the first part of lecture (ML-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc668602c44bc302af6f47876a0da1d2",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Modeling uncertainty\n",
    "\n",
    "In the lecture it is claimed that probabilities can summarize several factors:\n",
    "\n",
    "1. missing knowledge\n",
    "1. incapability to devise complete models of complex domains\n",
    "1. chance\n",
    "\n",
    "Think of an example for each of these points and explain how probabilities can be applied in modeling your example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "72f109d9312c8ee012453726ed0c9c61",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. only partially observed environment: \n",
    "2. weather prediction: \n",
    "3. coin toss: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e98ca903dd990da0c0dd014f43b3f280",
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Inference by enumeration\n",
    "\n",
    "Given the full joint distribution shown on (ML-11 slide 15), calculate the following:\n",
    "1. $P(\\neg toothache)$\n",
    "1. $P(cavity)$\n",
    "1. $P(toothache \\mid cavity)$\n",
    "1. $P(cavity \\mid toothache \\vee catch)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96c23f82ac2bbe8d6e71640d3430fd6c",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. $P( \\neg toothache) = 0.072+0.008+0.144+0.576 = 0.8$\n",
    "2. $P(cavity) = 0.108+0.012+0.072+0.008 = 0.2$\n",
    "3. $P(toothache|cavity) = \\frac{P(toothache\\wedge cavity)}{P(cavity)} = \\frac{0.108+0.012}{P(0.108+0.012+0.072+0.008} = 0.6$\n",
    "4. $P(cavity|toothache \\vee catch) = \\frac{P(cavity\\wedge toothache\\wedge catch)}{P(cavity\\vee catch)} =\\frac{0.108+0.012+0.072}{0.108+0.012+0.016+0.064+0.072+0.144}= 0.462$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ce4a21026801118cba8a59edeac6545",
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) Conditional probability\n",
    "\n",
    "For each of the following statements, either prove it is true or give a counterexample.\n",
    "1. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)\n",
    "1. If P (a | b, c) = P (a), then P (b | c) = P (b)\n",
    "1. If P (a | b) = P (a), then P (a | b, c) = P (a | c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b305930b2b6a6b420481a5f98bda610",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. true: $P(a,b,c) = P(a|b,c)*P(b|c)*P(c) = P(b|a,c)*P(a|c)*P(c) <=> P(b|c) = P(a|c)$\n",
    "2. false: \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "711f158a7b5412d3f8bdf93e090c717f",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### d) Independence and conditional independence\n",
    "\n",
    "\n",
    "It is quite often useful to consider the effect of some specific propositions in the\n",
    "context of some general background evidence that remains fixed, rather than in the complete\n",
    "absence of information. The following questions ask you to prove more general versions of\n",
    "the product rule and Bayes’ rule, with respect to some background evidence e:\n",
    "\n",
    "1. Prove the conditionalized version of the general product rule:\n",
    "$$P(X, Y \\mid e) = P(X \\mid Y, e)\\cdot P(Y \\mid e) .$$\n",
    "1. Prove the conditionalized version of Bayes’ rule:\n",
    "$$P(Y \\mid X, e) = \\frac{P(X \\mid Y, e)\\cdot P(Y \\mid e)}{P(X \\mid e)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d82e8ee6329c1cd2884acc53f7c1447",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. $P(X,Y|e) = P(X|Y,e)*P(Y|e) <=> P(X,Y,e) = P(X|Y,e)*P(Y,e) <=> P(X|Y,e) = P(X|Y,e)$\n",
    "2. $P(Y|X,e) = \\frac{P(X|Y,e)*P(Y|e)}{P(X|e)} <=> P(Y,X,e) = P(X|Y,e)*P(Y,e) <=> P(X|Y,e) = P(X|Y,e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcbbb6cd30e796ddab280cf98ed6c95d",
     "grade": false,
     "grade_id": "ex1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### e) Naive Bayes models\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of\n",
    "categories on the basis of the text it contains. Naive Bayes models are often used for this\n",
    "task. In these models, the query variable is the document category, and the “effect” variables\n",
    "are the presence or absence of each word in the language; the assumption is that words occur\n",
    "independently in documents, with frequencies determined by the document category.\n",
    "1. Explain precisely how such a model can be constructed, given as “training data” a set of documents that have been assigned to categories.\n",
    "1. Explain precisely how to categorize a new document.\n",
    "1. Is the conditional independence assumption reasonable? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f38aa21179d3265a3cf2547518cae962",
     "grade": true,
     "grade_id": "ex1e_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "1. The category is denoted as the cause and the absence or presence of words are the effect variables. Since the occurance of the words is conditionally independent of the presence of other words given the category a naive bayes model can be applied. One has to calculate the probability of the occurance of a word given a category for all pairings of words and categories. The product of these conditional probability times the probability of the category gives the joint probability of a category and the words.\n",
    "2. $P(C|w_1,...w_n) = \\frac{P(C,w_1,...w_n)}{P(D)} = \\frac{\\prod_iP(w_i|C)*P(C)}{(D)}$.\n",
    "3. It is a reasonable assumption since the probability of those words that have informational value and contribute to the categorization is rather not dependend on the occurance of other words given the category. In general words do not occur independently but with the additional information of the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a1d52b93c81da19b269e13a188b5371",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Bayes networks [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "227c0a791edd9f2b11e01a29959f43e4",
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Bayes networks\n",
    "\n",
    "Explain in your own words the idea of a Bayes network. How is conditional independence represented in such a network? How can the full joint distribution be regained from such a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "317483e3c43ea21a9d0b27e8cd2f19db",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "A Bayesian network is a graphical notation to express joint distributions by propositions on conditional probabilities. Conditional independence is given if no edge exists between the variables and their shared parental nodes are given. The full joint probability distribution can be regained by taking the product of the conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b56648be1657a979cd035a4522ac1047",
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) Independence in Bayes networks\n",
    "\n",
    "Consider the Bayes network in (ML-11 slide 32):\n",
    "1. If no evidence is observed, are Burglary and Earthquake independent? Prove this from the numerical semantics and from the topological semantics.\n",
    "1. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your answer by calculating whether the probabilities involved satisfy the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dbdbd8962f13ea2595b59c825f1c9a3b",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "    1. Burglary and Earthquake are independent since:\n",
    "        a) d-seperated: unobserved head-to-head thus path blocked and B an E are independent\n",
    "        b) their joint probability equals the product of their probabilities: $P(B,E) = P(B|E)*P(E) = P(B)*P(E)$\n",
    "    2. Burglary and Earthquake given alarm are not independent since:\n",
    "        a) not d-seperated: observed head-to-head thus path free and B and E ae not independent\n",
    "        b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "586bd37b4074e3039b48208105d82ced",
     "grade": false,
     "grade_id": "r02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part I)\n",
    "\n",
    "This part of the sheet is intended to revise some topics from the lecture, a second part is following on the next sheet. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89444826bba181874e0b85ac48fa4dfd",
     "grade": false,
     "grade_id": "ex01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 1: Concept Learning [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6dd50cc83285c1232c1195d63abffe81",
     "grade": false,
     "grade_id": "ex01a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Concept Learning\n",
    "\n",
    "What is Concept Learning? Is it supervised? Is it local?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f848ccaba8166e9f7484be68c7f298b",
     "grade": true,
     "grade_id": "ex01a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concept learning has the aim of being able to choose whether an example belongs to this concept or not. It is usually supervised and can be local(nearest neighbour) or global (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d737190c7f553a49494e7eb419fb34c",
     "grade": false,
     "grade_id": "ex01b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Find-S\n",
    "Describe the Find-S Algorithm in pseudo code. What is its inductive bias? What are its advantages and drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "18e819151f0265e4150ffa80886a3189",
     "grade": true,
     "grade_id": "ex01b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    initialize s as most specific hypothesis in H.\n",
    "    for each positive example d do:\n",
    "        for each attribute a in s do:\n",
    "            if a is not satisfied by x:\n",
    "                replace a in s by next most general constraint satisfied by x\n",
    "The inductive bias: hypothesis representation\n",
    "\n",
    "advantage: picks maximally specific hypothesis\n",
    "\n",
    "drawback: does not learn from negative examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "831be53b6f45c70670bc74332cda5e8a",
     "grade": false,
     "grade_id": "ex01c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hypotheses space\n",
    "\n",
    "What is the hypotheses space for Candidate-Elimination used in the lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6ee5c5afe2f8e55340f70b98d56d021f",
     "grade": true,
     "grade_id": "ex01c_solution",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is the space spanned by the most specific and most general hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5299e31e4c11bbbe108c8acdc632dfa7",
     "grade": false,
     "grade_id": "ex02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 2: Decision Trees [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a02502f7aa7e6b21ac550b63992ee4fb",
     "grade": false,
     "grade_id": "ex02a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Overfitting\n",
    "What is overfitting? How can it be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d3952e567abbde69a5fdffd4f5f3defa",
     "grade": true,
     "grade_id": "ex02a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It means that the learned tree performes better on the training data than another tree but worse on the validation set. This occurs due to learning of the noise in the training data such that the ability to generalize and also perform good on other data is lost.\n",
    "\n",
    "It can be avoided by checking for it and stopping the learning process if detected. Or one can later on perform some means to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1a0d5e1bc5af31544ce463bd6309c97",
     "grade": false,
     "grade_id": "ex02b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Pruning\n",
    "\n",
    "Name one method for pruning a decision tree and describe it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7cb89deabdf0ceeebcf18306b7b0e929",
     "grade": true,
     "grade_id": "ex02b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. reduced error pruning:\n",
    "    - greedily remove a node from the tree (increase performance on validation set)\n",
    "2. rule-post pruning:\n",
    "    - convert tree to rules\n",
    "    - for each rule: remove antecedants (increase performance on validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "581ce55f66d4fa663c711d4f6f8083a2",
     "grade": false,
     "grade_id": "ex02c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Information gain\n",
    "What are entropy and information gain? Provide explanation and formulae. How are they used in ID3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a03b8a2efba5138af0ef701ebc4c8079",
     "grade": true,
     "grade_id": "ex02c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. entropy: measure of impurity\n",
    "\n",
    "    $E(S) = -\\sum_{i} p_i * log(p_i)$\n",
    "2. information gain: expected reduce in entropy by sorting on A\n",
    "\n",
    "    $Gain(S,A) = E(S) - \\sum_{v\\in A}E(S_v)* \\frac{|S_v|}{|S|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2523c25781782f0b76adb0bb007f71a9",
     "grade": false,
     "grade_id": "ex03",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 3: Data Mining [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0854d423cad5f1d191f7fdc7fbf8a54",
     "grade": false,
     "grade_id": "ex03a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Missing values\n",
    "\n",
    "How can you deal with missing values? Name an important algorithm and explain how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50d5bcf42266796846c068c0305106d1",
     "grade": true,
     "grade_id": "ex03a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Missing values can be replaced by mean/median, value drawn from model, generated value from estimated distribution or by using Expectation-Maximization-Algorithm. \n",
    "\n",
    "    choose function for P\n",
    "    choose initial values for estimated parameters $\\theta_t$\n",
    "        while function $Q(\\theta,\\theta_t)$ does not meet convergence criterion\n",
    "            E-step: calculate new Q\n",
    "            M-step: maximize $Q$ with respect to new $\\theta_{t+1} = argmax_{\\theta}Q(\\theta,\\theta_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e718969e5c226ca2e25eb4ed3aaab07e",
     "grade": false,
     "grade_id": "ex03b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Outliers\n",
    "\n",
    "What are outliers? Can we detect them? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa13ef09df2fc9dfbf0cf5f037441b93",
     "grade": true,
     "grade_id": "ex03b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Outliers are those values that do not ly within what is defined as regular. They can be detected by $z-values$ which define those values being normal that satisfy: $\\frac{|x-\\mu|}{\\sigma} < 3$. A suitable algorithm would be the Rosner-test which iteratively removes outliers until none are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e8b28527cd05f7351675347bbf610a7",
     "grade": false,
     "grade_id": "ex03c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) \n",
    "What does the Q-function express in the EM algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "976ec12fd1ac5e884bb64e6967a0ed8c",
     "grade": true,
     "grade_id": "ex03c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Q funtion is the expected log likelihood funtion: $$Q(\\theta\\mid\\theta_{t}) = E_{h\\mid x,\\theta_t}[\\log L(\\theta,x,h)] = \\int P(h\\mid x,\\theta_t)\\cdot \\log P(h\\mid x,\\theta)\\operatorname{d} h+\\log P(x\\mid\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24ba408a5c6707d5fd9540324b224375",
     "grade": false,
     "grade_id": "ex04",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 4: Clustering [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "373adcc0111aa5f139a3bd5b7f74e062",
     "grade": false,
     "grade_id": "ex04a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Clustering\n",
    "\n",
    "Explain the difference between single-linkage and complete-linkage clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0e3f4c5c1a2fce02a2671e99ee34746",
     "grade": true,
     "grade_id": "ex04a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Single-linkage clustering employs the minimum distance between two clusters whereas complete-linkage clustering minimizes the maximum distance. Thereby they lead to chaining or the preference of compact clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d72601f8cd17c51c838b96fbb3f5d5aa",
     "grade": false,
     "grade_id": "ex04b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Metrics\n",
    "\n",
    "Name three different distance measures and briefly explain them. Check the metric axioms for one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1813d29faac7affb585aa5f0dc0798f5",
     "grade": true,
     "grade_id": "ex04b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. p-norm: $\\sqrt[p](\\sum_{i=1}^{L}|x_i-y_i|^p)$ expresses with respective p-values the city-block($p=1$), euclidean($p=2$) and maximum distance($p\\to \\inf$)\n",
    "2. Jaccard: $J(A,B) = \\frac{|A\\cap B|}{|A\\cup B|}$ binary attributes\n",
    "\n",
    "symmetry: $d(x,y) = \\sqrt{\\sum_{i}(x_i-y_i)^2} = \\sqrt{\\sum_{i}(y_i-x_i)^2} = d(y,x)$\n",
    "\n",
    "coincidence: $d(x,x) = \\sqrt{\\sum_{i}(x_i-x_i)^2} = 0$\n",
    "\n",
    "triangle: $d(x,y)+d(y,z) = \\sqrt{\\sum_{i}(x_i-y_i)^2} + \\sqrt{\\sum_{i}(y_i-z_i)^2} >= \\sqrt{\\sum_{i}(x_i-y_i)^2+(y_i-z_i)^2} >= \\sqrt{\\sum_{i}(x_i-z_i)^2} = d(x,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "539b0a8be1e2aeddcca1933b7c3e7f04",
     "grade": false,
     "grade_id": "ex04c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Mixture models\n",
    "\n",
    "What is a mixture model? Explain. Can you provide a formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "718e7afbc96985c166fa509dbc957246",
     "grade": true,
     "grade_id": "ex04c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gaussian mixture models?\n",
    "Composition of the following things: \n",
    "- K: number of mixture components\n",
    "- N: number of observations\n",
    "- $\\theta_i$: distribution parameters of component i (like $\\mu, \\sigma$)\n",
    "- $g_i$: prior probability of component i, mixture weight (sum up to 1)\n",
    "- $x_j$: observation j\n",
    "- $z_j$: component of observation j\n",
    "\n",
    "Probability given data x belongs to component k:\n",
    "$p(k|x) = \\frac{g_k*N(\\theta_k)}{\\sum_{i=1}^Kg_i*N(\\theta_i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3850c1608d8fe96f63823008d11c71b2",
     "grade": false,
     "grade_id": "ex05",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 5: Dimension Reduction [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a033c54e38f456ac44116663f495beea",
     "grade": false,
     "grade_id": "ex05a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Visualization\n",
    "\n",
    "Name three different data visualization techniques to visualize high dimensional data. Explain one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9d23c78e45b73046de3c8529c351df01",
     "grade": true,
     "grade_id": "ex05a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **glyphs**: map each dimension onto dimension of geometrical figure(stars, parallel).\n",
    "2. **scatterplots**: 2d-projection of data for each combination of dimensions.\n",
    "3. **Chernoff-faces**: map each dimension onto facial feature.\n",
    "4. **projection pursuit**: project onto 1-3 dimensions that exhibit intersting structure (non-Gaussian, high variance, clusters)\n",
    "5. **multidimensional scaling**: find lower dimensional manifold that perserves structure (distance between data points well approximated by projected data points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8802c24ea5a82ef6a8542b0dc33de4b2",
     "grade": false,
     "grade_id": "ex05b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) PCA\n",
    "\n",
    "Draw a few data points (ASCII arts or on a sheet of paper) and mark the principal components. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e43203e4824dae30c77cd382c12812",
     "grade": true,
     "grade_id": "ex05b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ba0948130b01d9ab0267c8007d9312d",
     "grade": false,
     "grade_id": "ex05c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Covariance matrix\n",
    "What does a covariance matrix express? How is it computed from data? How is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "90cb090ef14311e4ea69d41da8684682",
     "grade": true,
     "grade_id": "ex05c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Covariance matrix includes covariances between all pairs of variables. $C=(D-\\mu)^{-1}*(D-\\mu)$ where D is the data matrix and $\\mu$ is the mean vector of the data.\n",
    "\n",
    "In PCA the covariance matrix is used to compute the eigenvalues which eigenvectors are the principle components."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
