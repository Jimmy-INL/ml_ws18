{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9e7d36d014ba44abbf2fe8283bbe76c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f06f9d6c2384d2f2a11309031ea33b68",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 24, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c989ef972a476392e000cb61e20d673f",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Uncertainty and probability [6 Points]\n",
    "\n",
    "This exercise will focus on concepts introduced in the first part of lecture (ML-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc668602c44bc302af6f47876a0da1d2",
     "grade": false,
     "grade_id": "ex1a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### a) Modeling uncertainty\n",
    "\n",
    "In the lecture it is claimed that probabilities can summarize several factors:\n",
    "\n",
    "1. missing knowledge\n",
    "1. incapability to devise complete models of complex domains\n",
    "1. chance\n",
    "\n",
    "Think of an example for each of these points and explain how probabilities can be applied in modeling your example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "72f109d9312c8ee012453726ed0c9c61",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "1. The color of an unknown species of frog. The color is certainly not random (think of evolution), but unknown.  One could calculate a probability for the color by using all colors of known frogs and weighting them according how related they are genetically and how related the habitat they are living in is.\n",
    "\n",
    "2. The weather forecast. It is nearly impossible to account for all relevant factors to forecast the weather. But those factors are also not random. They can be modeled with reducing the domain of factors and outcomes.\n",
    "\n",
    "3. Chance is difficult to find. It is rather a philosophical concept and in our society I would say it is similar to a religious belief. In particle physics it is stated that some aspects can only be modeled by chance, since knowledge of the process will change the process. \n",
    "\n",
    "I think probabilities are in general a confession to a boundary of our knowledge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e98ca903dd990da0c0dd014f43b3f280",
     "grade": false,
     "grade_id": "ex1b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### b) Inference by enumeration\n",
    "\n",
    "Given the full joint distribution shown on (ML-11 slide 15), calculate the following:\n",
    "1. $P(\\neg toothache)$\n",
    "1. $P(cavity)$\n",
    "1. $P(toothache \\mid cavity)$\n",
    "1. $P(cavity \\mid toothache \\vee catch)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "96c23f82ac2bbe8d6e71640d3430fd6c",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "1. $P(\\neg toothache) = 0.072 + 0.008 + 0.144 + 0.576 = 0.8$\n",
    "1. $P(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2$\n",
    "1. $P(toothache \\mid cavity) = \\frac{0.108 + 0.012}{0.2} = 0.6$\n",
    "1. $P(cavity \\mid toothache \\vee catch) = \\frac{0.108 + 0.012 + 0.072}{0.108 + 0.012 + 0.016 + 0.064 + 0.072 + 0.144} = 0.46$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ce4a21026801118cba8a59edeac6545",
     "grade": false,
     "grade_id": "ex1c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### c) Conditional probability\n",
    "\n",
    "For each of the following statements, either prove it is true or give a counterexample.\n",
    "1. If P (a | b, c) = P (b | a, c), then P (a | c) = P (b | c)\n",
    "1. If P (a | b, c) = P (a), then P (b | c) = P (b)\n",
    "1. If P (a | b) = P (a), then P (a | b, c) = P (a | c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b305930b2b6a6b420481a5f98bda610",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "On paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "711f158a7b5412d3f8bdf93e090c717f",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### d) Independence and conditional independence\n",
    "\n",
    "\n",
    "It is quite often useful to consider the effect of some specific propositions in the\n",
    "context of some general background evidence that remains fixed, rather than in the complete\n",
    "absence of information. The following questions ask you to prove more general versions of\n",
    "the product rule and Bayes’ rule, with respect to some background evidence e:\n",
    "\n",
    "1. Prove the conditionalized version of the general product rule:\n",
    "$$P(X, Y \\mid e) = P(X \\mid Y, e)\\cdot P(Y \\mid e) .$$\n",
    "1. Prove the conditionalized version of Bayes’ rule:\n",
    "$$P(Y \\mid X, e) = \\frac{P(X \\mid Y, e)\\cdot P(Y \\mid e)}{P(X \\mid e)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d82e8ee6329c1cd2884acc53f7c1447",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "On paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcbbb6cd30e796ddab280cf98ed6c95d",
     "grade": false,
     "grade_id": "ex1e",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### e) Naive Bayes models\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of\n",
    "categories on the basis of the text it contains. Naive Bayes models are often used for this\n",
    "task. In these models, the query variable is the document category, and the “effect” variables\n",
    "are the presence or absence of each word in the language; the assumption is that words occur\n",
    "independently in documents, with frequencies determined by the document category.\n",
    "1. Explain precisely how such a model can be constructed, given as “training data” a set of documents that have been assigned to categories.\n",
    "1. Explain precisely how to categorize a new document.\n",
    "1. Is the conditional independence assumption reasonable? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f38aa21179d3265a3cf2547518cae962",
     "grade": true,
     "grade_id": "ex1e_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "Not yet covered in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a1d52b93c81da19b269e13a188b5371",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Bayes networks [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "227c0a791edd9f2b11e01a29959f43e4",
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### a) Bayes networks\n",
    "\n",
    "Explain in your own words the idea of a Bayes network. How is conditional independence represented in such a network? How can the full joint distribution be regained from such a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "317483e3c43ea21a9d0b27e8cd2f19db",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "Bayes networks are graphical representations of probabilities. Each node represents a proposition and the arrows are dependencies from one proposition to another. Probabilities of a proposition are conditioned by all proposition which map with an arrow on this proposition. The joint probability of the graph is determined by all conditional probabilities of the graph as a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b56648be1657a979cd035a4522ac1047",
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    }
   },
   "source": [
    "### b) Independence in Bayes networks\n",
    "\n",
    "Consider the Bayes network in (ML-11 slide 32):\n",
    "1. If no evidence is observed, are Burglary and Earthquake independent? Prove this from the numerical semantics and from the topological semantics.\n",
    "1. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your answer by calculating whether the probabilities involved satisfy the definition of conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dbdbd8962f13ea2595b59c825f1c9a3b",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2.0,
     "schema_version": 1.0,
     "solution": true
    }
   },
   "source": [
    "These questions are answered with the d - separation property. The probability of Burglary and Earthquake are connected by a head-to-head connection on Alarm. Therefor they are dependent when Alarm is observed or any descendent node of Alarm (JohnCalls, MarryCalls) is observed. Otherwise they are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "586bd37b4074e3039b48208105d82ced",
     "grade": false,
     "grade_id": "r02",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part I)\n",
    "\n",
    "This part of the sheet is intended to revise some topics from the lecture, a second part is following on the next sheet. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89444826bba181874e0b85ac48fa4dfd",
     "grade": false,
     "grade_id": "ex01",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 1: Concept Learning [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6dd50cc83285c1232c1195d63abffe81",
     "grade": false,
     "grade_id": "ex01a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Concept Learning\n",
    "\n",
    "What is Concept Learning? Is it supervised? Is it local?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f848ccaba8166e9f7484be68c7f298b",
     "grade": true,
     "grade_id": "ex01a_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concept learning is a procedure to infer the underlying shaping of features for which the data is classified positive or negative. Since Concept learning is based on data being either positive or negative classified it is supervised. Concept learning can be local or global depending on the learning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3d737190c7f553a49494e7eb419fb34c",
     "grade": false,
     "grade_id": "ex01b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Find-S\n",
    "Describe the Find-S Algorithm in pseudo code. What is its inductive bias? What are its advantages and drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "18e819151f0265e4150ffa80886a3189",
     "grade": true,
     "grade_id": "ex01b_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Set hypothesis to the first data.\n",
    "\n",
    "For each positive classified data in the dataset:\n",
    "\n",
    "    For all features in the hypothesis:\n",
    "    \n",
    "        if feature of hypothesis not same or not more general then feature of data:\n",
    "        \n",
    "            make feature more general \n",
    "            \n",
    "return hypothesis\n",
    "\n",
    "\n",
    "The inductive bias is first the bias of the representation of the hypothesis. Each feature is either a certain shaping or more general but no disjunction of different shapings. Secondly the inductive bias of find-s algorithm is that it only learns from positive examples. \n",
    "\n",
    "\n",
    "Advantages are very fast training. Drawbacks are that it does not uses the knowledge of negative classified data. Also the simple representation of concept learning can be seen as a draw back since the underlying hypothesis could have to shapings in one feature which should be classified positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "831be53b6f45c70670bc74332cda5e8a",
     "grade": false,
     "grade_id": "ex01c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hypotheses space\n",
    "\n",
    "What is the hypotheses space for Candidate-Elimination used in the lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6ee5c5afe2f8e55340f70b98d56d021f",
     "grade": true,
     "grade_id": "ex01c_solution",
     "locked": false,
     "points": 0.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hypothesis space of candidate elimination algorithm is limited by a general and specific bound. The hypothesis is more general than the specific boundary and more general than the specific boundary. The specific boundary has to be more general or as general as all positive examples and more specific than the general boundary. The general boundary has to differ from all negative examples and must be more general than the specific boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5299e31e4c11bbbe108c8acdc632dfa7",
     "grade": false,
     "grade_id": "ex02",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 2: Decision Trees [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a02502f7aa7e6b21ac550b63992ee4fb",
     "grade": false,
     "grade_id": "ex02a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Overfitting\n",
    "What is overfitting? How can it be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d3952e567abbde69a5fdffd4f5f3defa",
     "grade": true,
     "grade_id": "ex02a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfitting occurs when the generated hypothesis includes structure which is due to noise in the training data. Overfitting can be tested by measuring performance of the  prediction of the hypothesis on data which was not part of the training (test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1a0d5e1bc5af31544ce463bd6309c97",
     "grade": false,
     "grade_id": "ex02b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Pruning\n",
    "\n",
    "Name one method for pruning a decision tree and describe it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7cb89deabdf0ceeebcf18306b7b0e929",
     "grade": true,
     "grade_id": "ex02b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One method is reduced error pruning. Reduced error pruning removes subtrees of the tree to greedily achieve a better performance on the validation data set. It ends when removing subtrees will decrease the performance.\n",
    "Another method is rule post pruning. Rule post pruning works on the transformed decision tree to a set of rules. It removes any precondition of the rules (independently) which do not result in better performance. Afterwards it sorts the rule according to their accuracy on the validation training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "581ce55f66d4fa663c711d4f6f8083a2",
     "grade": false,
     "grade_id": "ex02c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Information gain\n",
    "What are entropy and information gain? Provide explanation and formulae. How are they used in ID3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a03b8a2efba5138af0ef701ebc4c8079",
     "grade": true,
     "grade_id": "ex02c_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Entropy is a measure which is highest when events are equally probable. It is calculated by the negative sum of the product of each probability and the logarithm of that probability. Information gain is a formula based on entropy. It is calculated by the entropy of a current leaf in a tree subtracted by the sum of the entropy of the features (branches of the tree) of another class. In ID3 it is used to choose which class comes next (class with highest information gain) on a current node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2523c25781782f0b76adb0bb007f71a9",
     "grade": false,
     "grade_id": "ex03",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 3: Data Mining [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0854d423cad5f1d191f7fdc7fbf8a54",
     "grade": false,
     "grade_id": "ex03a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Missing values\n",
    "\n",
    "How can you deal with missing values? Name an important algorithm and explain how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "50d5bcf42266796846c068c0305106d1",
     "grade": true,
     "grade_id": "ex03a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can either leave out data points with missing values or replace them by some value. Leaving them out might lead to high data loss in case of data vectors with many components. Replacing the missing value will lead in the best case not to biased dat. \n",
    "\n",
    "The Expected Maximization algorithm is an algorithm which can be used to infer values for missing data. It is used by computing a Gaussian and maximizing the probability for the mean and the covariance matrix. Each iteration each data point is weighted by the probability of the Gaussian and with those values the mean and Covariance matrix are updated. Missing values are inferred from this Gaussian each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e718969e5c226ca2e25eb4ed3aaab07e",
     "grade": false,
     "grade_id": "ex03b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Outliers\n",
    "\n",
    "What are outliers? Can we detect them? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa13ef09df2fc9dfbf0cf5f037441b93",
     "grade": true,
     "grade_id": "ex03b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Outliers are data points which are unexpected far away from the majority of the data points. One algorithm to detect outliers is the Rosners Test. The Rosner test calculates each iteration z-values for each data point (by computing mean and standard deviation). Data points above a certain z-value are removed. This iteration is repeated until there are no z-values anymore above the certain z-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e8b28527cd05f7351675347bbf610a7",
     "grade": false,
     "grade_id": "ex03c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) \n",
    "What does the Q-function express in the EM algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "976ec12fd1ac5e884bb64e6967a0ed8c",
     "grade": true,
     "grade_id": "ex03c_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Q function expresses the average likelihood. It is tried to maximize this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24ba408a5c6707d5fd9540324b224375",
     "grade": false,
     "grade_id": "ex04",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 4: Clustering [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "373adcc0111aa5f139a3bd5b7f74e062",
     "grade": false,
     "grade_id": "ex04a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Clustering\n",
    "\n",
    "Explain the difference between single-linkage and complete-linkage clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0e3f4c5c1a2fce02a2671e99ee34746",
     "grade": true,
     "grade_id": "ex04a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Single-linkage and complete-linkage clustering are both agglomerative clustering. In single linkage clustering the minimum distance function is applied (shortest distance to point in cluster) and in complete linkage clustering the maximum distance is applied (shortest distance to farthest point in cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d72601f8cd17c51c838b96fbb3f5d5aa",
     "grade": false,
     "grade_id": "ex04b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Metrics\n",
    "\n",
    "Name three different distance measures and briefly explain them. Check the metric axioms for one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1813d29faac7affb585aa5f0dc0798f5",
     "grade": true,
     "grade_id": "ex04b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Euclidian distance, the Manhattan norm and Chebyshev distance are 3 distance measures. They fulfill the axioms of symmetry, coincidence and triangular equation and therefore are distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "539b0a8be1e2aeddcca1933b7c3e7f04",
     "grade": false,
     "grade_id": "ex04c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Mixture models\n",
    "\n",
    "What is a mixture model? Explain. Can you provide a formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "718e7afbc96985c166fa509dbc957246",
     "grade": true,
     "grade_id": "ex04c_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A mixture model assigns points to clusters according to a probability (soft clustering). The EM algorithm can be used to compute K Gaussians which represent soft clusters.  (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3850c1608d8fe96f63823008d11c71b2",
     "grade": false,
     "grade_id": "ex05",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 5: Dimension Reduction [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a033c54e38f456ac44116663f495beea",
     "grade": false,
     "grade_id": "ex05a",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Visualization\n",
    "\n",
    "Name three different data visualization techniques to visualize high dimensional data. Explain one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9d23c78e45b73046de3c8529c351df01",
     "grade": true,
     "grade_id": "ex05a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scatter plot matrix, Glyphs, Chernov faces. The Scatter plot matrices are used to plot all different combinations of axis in a 2d plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8802c24ea5a82ef6a8542b0dc33de4b2",
     "grade": false,
     "grade_id": "ex05b",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) PCA\n",
    "\n",
    "Draw a few data points (ASCII arts or on a sheet of paper) and mark the principal components. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8e43203e4824dae30c77cd382c12812",
     "grade": true,
     "grade_id": "ex05b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On paper. The PCA are those vectors which if the data projected on them capture the highest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ba0948130b01d9ab0267c8007d9312d",
     "grade": false,
     "grade_id": "ex05c",
     "locked": true,
     "schema_version": 1.0,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Covariance matrix\n",
    "What does a covariance matrix express? How is it computed from data? How is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "90cb090ef14311e4ea69d41da8684682",
     "grade": true,
     "grade_id": "ex05c_solution",
     "locked": false,
     "points": 1.0,
     "schema_version": 1.0,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Covariance matrix is computed by computing the variance of each feature of the data space and covariance between all features of the data space over all data points. It expresses the variance within and between the features. The PCA are computed on the Covariance matrix by spectral decomposition (since the covariance matrix is symmetric spectral decomposition is guaranteed).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
